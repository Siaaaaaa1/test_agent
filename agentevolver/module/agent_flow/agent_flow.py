import time
import os

from loguru import logger

from agentevolver.client.em_client import EMClient
from agentevolver.client.env_client import EnvClient
from agentevolver.module.agent_flow.base_agent_flow import BaseAgentFlow
from agentevolver.utils.utils import convert_tool_to_user_message
from agentevolver.schema.trajectory import Reward, Trajectory
from best_logger import register_logger, print_dict, print_listofdict
from agentevolver.module.context_manager.cmt_linear import Linear_CMT, ExtendedMessage
from agentevolver.module.context_manager.cmt_linear_think import LinearThinkCMT
from agentevolver.module.context_manager.cmt_context_clip import SelfContextClipCMT
from agentevolver.module.agent_flow.reward_calculator import RewardCalculator
from typing import Any, Dict, List, Union, Optional
import threading
from agentevolver.module.exp_manager.exp_manager import TrajExpConfig, ExperienceWorker

# å…¨å±€é”ï¼Œç”¨äºæ§åˆ¶æ—¥å¿—ç”Ÿæˆçš„çº¿ç¨‹å®‰å…¨
log_generate_lock = threading.Lock()

class AgentFlow(BaseAgentFlow):
    """
    AgentFlow ç±»ï¼šå®ç°äº†å…·ä½“çš„ Agent ä¸ç¯å¢ƒäº¤äº’çš„å¾ªç¯é€»è¾‘ (Think-Act Loop)ã€‚
    ç»§æ‰¿è‡ª BaseAgentFlowã€‚
    """

    def __init__(self, reward_calculator:Optional[RewardCalculator]=None, **kwargs):
        """
        åˆå§‹åŒ– AgentFlow å®ä¾‹ã€‚

        Args:
            reward_calculator (Optional[RewardCalculator]): å¯é€‰çš„å¥–åŠ±è®¡ç®—å™¨ã€‚å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨å®ƒæ¥è®¡ç®—æœ€ç»ˆå¥–åŠ±ï¼ˆå¦‚åŸºäº LLM çš„è¯„åˆ†ï¼‰ï¼›å¦åˆ™ä½¿ç”¨ç¯å¢ƒè‡ªå¸¦çš„è¯„ä¼°å‡½æ•°ã€‚
            **kwargs: ä¼ é€’ç»™åŸºç±»çš„å…¶ä»–å…³é”®å­—å‚æ•° (config, tokenizer, llm_chat_fn)ã€‚
        """
        super().__init__(**kwargs)  # â­ è°ƒç”¨åŸºç±»æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–é…ç½®ã€Tokenizer å’Œ LLM æ¥å£
        self._reward_calculator = reward_calculator
        # self._enable_context_generator=self.config.experience_maker.enable_context_generator

        # é¢„å…ˆç¼–ç ç”¨æˆ·æŒ‡ä»¤å’ŒåŠ©æ‰‹å›å¤çš„æ¨¡æ¿ Tokenï¼Œç”¨äºåç»­å¤„ç†
        self.instruction_template_ids = self.tokenizer.encode("user\n")  
        self.response_template_ids = self.tokenizer.encode("assistant\n")  
        
        # sparse æ ‡å¿—ï¼šæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ç¨€ç–å¥–åŠ±ï¼ˆSparse Rewardï¼Œé€šå¸¸ 0 æˆ– 1ï¼‰
        self.sparse = self.config.actor_rollout_ref.rollout.sparse  
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (CMT) å®ä¾‹å ä½ç¬¦
        self.cmt: Union[Linear_CMT, LinearThinkCMT] = None
        
        # æ§åˆ¶å°è°ƒè¯•æ¨¡å¼å¼€å…³
        self.console_debug_mode: bool = self.config.actor_rollout_ref.rollout.debug_llm_io
        
        # åˆå§‹åŒ–ç»éªŒå·¥ä½œè€…ï¼Œè´Ÿè´£ç®¡ç†ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰ç›¸å…³çš„é€»è¾‘
        self.exp_worker = ExperienceWorker(config=self.config)


    def execute(self, context_manager, init_messages: List[dict], env: EnvClient, instance_id: str, tmux, stop, thread_index, task_id, traj_exp_config, data_id="", rollout_id="", query="", **kwargs) -> Linear_CMT:
        """
        æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ï¼šç®¡ç† AI Agent ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œç”Ÿæˆè½¨è¿¹ã€å¤„ç†ç»éªŒå¹¶è®¡ç®—å¥–åŠ±ã€‚

        Args:
            context_manager (ContextManager): å½“å‰ä»»åŠ¡çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (è´Ÿè´£ç»´æŠ¤ Prompt History)ã€‚
            init_messages (List[dict]): ä»»åŠ¡çš„åˆå§‹æ¶ˆæ¯åˆ—è¡¨ (é€šå¸¸åŒ…å« System Prompt å’Œ User Query)ã€‚
            env (EnvClient): ç¯å¢ƒå®¢æˆ·ç«¯ï¼Œç”¨äºä¸è¿œç¨‹ç¯å¢ƒé€šä¿¡ã€‚
            instance_id (str): å½“å‰è¿è¡Œçš„ç¯å¢ƒå®ä¾‹ IDã€‚
            tmux (dict): ç”¨äºè·¨çº¿ç¨‹çŠ¶æ€ç›‘æ§çš„å­—å…¸ (è®°å½• step, token ç­‰)ã€‚
            stop (list): åœæ­¢æ ‡å¿—åˆ—è¡¨ï¼Œç”¨äºæ£€æŸ¥æ˜¯å¦åº”è¯¥æå‰ç»ˆæ­¢å½“å‰çº¿ç¨‹ã€‚
            thread_index (int): å½“å‰çº¿ç¨‹çš„ç´¢å¼•ã€‚
            task_id (str): ä»»åŠ¡ IDã€‚
            traj_exp_config (TrajExpConfig): è½¨è¿¹çš„ç»éªŒé…ç½® (æ§åˆ¶æ˜¯å¦æ’å…¥å†å²ç»éªŒ)ã€‚
            data_id (str, optional): æ•°æ® IDã€‚é»˜è®¤ä¸º ""ã€‚
            rollout_id (str, optional): Rollout IDã€‚é»˜è®¤ä¸º ""ã€‚
            query (str, optional): æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚é»˜è®¤ä¸º ""ã€‚
            **kwargs: å…¶ä»–å‚æ•°ã€‚

        Returns:
            Linear_CMT: æ‰§è¡Œå®Œæˆåçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¹è±¡ (æœ¬è´¨ä¸Šä¹Ÿæ˜¯ç”Ÿæˆçš„ Trajectory)ã€‚
        """
        self.cmt = context_manager
        
        # é’ˆå¯¹ Qwen3 æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†ï¼šæ·»åŠ  /no_think æ ‡è®°ä»¥ç¦ç”¨æ€ç»´é“¾ï¼ˆå¦‚æœé…ç½®è¦æ±‚ï¼‰
        add_nothink = self.config.actor_rollout_ref.rollout.use_qwen3 

        # 1. ğŸš€ åˆå§‹åŒ–æ¶ˆæ¯å’Œç»éªŒ
        # å°†æœ¬æ¬¡ä»»åŠ¡çš„ query æ³¨å…¥é…ç½®
        traj_exp_config.query = query
        
        # è°ƒç”¨ exp_worker å¤„ç†åˆå§‹æ¶ˆæ¯ï¼Œå¯èƒ½ä¼šåœ¨ Prompt ä¸­æ’å…¥æ£€ç´¢åˆ°çš„ç›¸å…³ç»éªŒ (RAG / Few-shot)
        init_messages, traj_exp_config = self.exp_worker.manage_rollout_context(
                init_messages=init_messages,
                traj_exp_config=traj_exp_config
                )
        
        # å°†ç»éªŒé…ç½®å…ƒæ•°æ®ä¿å­˜åˆ°è½¨è¿¹ä¸­
        self.cmt.metadata["task_train_exp_mode"] = traj_exp_config.train_mode
        self.cmt.metadata["add_exp"] = traj_exp_config.add_exp
        self.cmt.metadata["experience_list"] = traj_exp_config.experience_list
        
        # å°†å¤„ç†åçš„åˆå§‹æ¶ˆæ¯ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­
        self.cmt.save_init_input(init_messages, add_nothink)

        request_id: str = ""
        err_in_generating = False
        err_in_env = False
        
        # ---------------- äº¤äº’å¾ªç¯ (ReAct Loop) ----------------
        for act_step in range(self.max_steps):
            # 2. ğŸ”„ æ›´æ–°çº¿ç¨‹è¿›åº¦
            tmux['step'][thread_index] = act_step
            # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°å¤–éƒ¨åœæ­¢ä¿¡å· (ä¾‹å¦‚å…¶ä»–çº¿ç¨‹å·²ç»æ‰¾åˆ°äº†ç­”æ¡ˆï¼Œä¸éœ€è¦å†è·‘äº†)
            if (stop is not None) and stop[thread_index]: 
                self.cmt.discarded = True
                break

            # 3. â®ï¸ å‡†å¤‡ä¸Šä¸‹æ–‡ (Prompt)
            try:
                # è·å– LLM çš„è¾“å…¥å†å²
                step_input_message_arr = self.cmt.prepare_next_llm_context()  
            except Exception as e:
                # å¦‚æœæ„å»º Prompt å¤±è´¥ï¼Œæ‰“å°å½“å‰çŠ¶æ€ä»¥ä¾¿è°ƒè¯•
                print_listofdict(self.cmt.to_role_content(self.cmt.full_context), mod='exception', header="Before Crash")
                raise e

            # 4. âš ï¸ æ£€æŸ¥ Token æº¢å‡º
            is_safe: bool = self.cmt.check_context_token_num_safe(step_input_message_arr)  
            if not is_safe:
                logger.warning(f"Token overflow detected at step {act_step}. Current token count exceeds the limit.")
                self.cmt.is_terminated = False # æ ‡è®°ä¸ºæœªå®Œæˆ
                break

            # 5. ğŸ¤– è°ƒç”¨ LLM (Think/Act)
            # å‘é€è¯·æ±‚ç»™ LLMï¼Œè·å–å›å¤ (content)
            llm_output = self.llm_chat_fn(step_input_message_arr, request_id=request_id)  
            
            # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
            if (stop is not None) and stop[thread_index]:  
                self.cmt.discarded = True
                break

            # 6. ğŸ’¾ ä¿å­˜ LLM è¾“å‡º
            # å°† LLM çš„å›å¤è®°å½•åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­
            self.cmt.save_llm_output(llm_output, input_msg_ref=step_input_message_arr)  
            # æ›´æ–°ç”Ÿæˆçš„ Token ç»Ÿè®¡
            tmux['token'][thread_index] += self.cmt.generated_token_cnt

            # 7. ğŸŒ ä¸ç¯å¢ƒäº¤äº’ (Environment Interaction)
            try:
                # å‡†å¤‡å‘é€ç»™ç¯å¢ƒçš„åŠ¨ä½œ (ä» LLM è¾“å‡ºä¸­æå–ä»£ç æˆ–æŒ‡ä»¤)
                action_content = self.cmt.prepare_world_interaction()
                # å‘é€ step è¯·æ±‚ç»™ç¯å¢ƒå®¢æˆ·ç«¯
                env_output = env.step(instance_id, {"content": action_content, "role": "assistant"})  
                
                # ç¡®ä¿ç¯å¢ƒè¿”å›æ ¼å¼æ­£ç¡®
                assert len(env_output['state'])==1
                env_output["state"] = env_output["state"][0]
                
                # å¦‚æœç¯å¢ƒè¿”å›çš„æ˜¯ Tool Role (OpenAI æ ¼å¼)ï¼Œè½¬æ¢ä¸º User Message (Qwen/é€šç”¨æ ¼å¼)
                if env_output["state"]["role"] == "tool":
                    env_output["state"] = convert_tool_to_user_message(env_output["state"], self.tokenizer, format="qwen")
                
                # æ§åˆ¶å°è°ƒè¯•è¾“å‡º
                if self.console_debug_mode:
                    print_listofdict(
                        step_input_message_arr +
                        [{'role': 'llm_latest', 'content': llm_output['content']}] +
                        [{'role': 'env',        'content': env_output["state"]['content']}]
                    , mod='c')
            except Exception as e:
                # æ•è·ç¯å¢ƒäº¤äº’å¼‚å¸¸
                logger.bind(exception=True).exception(f"call env.step error with {e}")
                err_in_env = True
                self.cmt.is_terminated = False # å‘ç”Ÿé”™è¯¯ï¼Œæ ‡è®°ä¸ºæœªå®Œæˆ
                # æ„é€ ä¸€ä¸ªé”™è¯¯çš„ Observation åé¦ˆç»™ Agent (æˆ–è€…ç›´æ¥ç»ˆæ­¢)
                state = {"content": str(e), "role": "user"}
                env_output = {
                    "reward": 0,
                    "is_terminated": True,
                    "state": state,
                }

            # 8. ğŸ“¥ ä¿å­˜ç¯å¢ƒè¾“å‡º (Observation)
            state = env_output["state"]
            # ç§»é™¤ä¸éœ€è¦çš„ tool_calls å­—æ®µ
            state.pop('tool_calls', None)
            # å°†ç¯å¢ƒçš„åé¦ˆè®°å½•åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­
            self.cmt.save_env_output(state, input_msg_ref=step_input_message_arr, add_nothink=add_nothink)  

            # 9. ğŸ”š åˆ¤æ–­ä»»åŠ¡æ˜¯å¦ç»ˆæ­¢
            self.cmt.is_terminated = env_output["is_terminated"]
            if self.cmt.is_terminated or err_in_env:
                break
        
        # ---------------- å¾ªç¯ç»“æŸ ----------------

        # æ ‡è®°çº¿ç¨‹çŠ¶æ€ä¸ºå·²å®Œæˆ
        tmux['step'][thread_index] = -1

        # 10. ğŸ† è®¡ç®—å¥–åŠ± (Reward Calculation)
        if self._reward_calculator is not None:
            # å¦‚æœé…ç½®äº†é«˜çº§å¥–åŠ±è®¡ç®—å™¨ (å¦‚ LLM-as-a-Judge)ï¼Œä½¿ç”¨å®ƒ
            grader_res = self._reward_calculator.calculate_reward(self.cmt, env, instance_id)  
            score = grader_res["score"] 
            reason = grader_res["reason"] or "No reason provided."
        else:
            # å¦åˆ™ä½¿ç”¨ç¯å¢ƒè‡ªå¸¦çš„è¯„ä¼°å‡½æ•° (é€šå¸¸æ˜¯ Outcome Reward)
            score = env.evaluate(instance_id, params={"sparse": self.sparse})  
            reason = "Outcome 1 = success, 0 = failure."

        # è®¡ç®—æˆåŠŸç‡ (é€šå¸¸ score >= 1 è§†ä¸ºæˆåŠŸ)
        if score >= 1: success_rate = 1.0
        else: success_rate = 0.0

        # å°†å¥–åŠ±ä¿¡æ¯æ‰“åŒ…å¹¶ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­
        # madness æ˜¯æŸç§è¡¡é‡ Agent è¡Œä¸ºç–¯ç‹‚ç¨‹åº¦æˆ–ä¸å¯æ§ç¨‹åº¦çš„æŒ‡æ ‡
        self.cmt.reward = Reward(outcome=score, success_rate=success_rate, madness=self.cmt.compute_madness(), description=reason)  
        
        # å¯¹å¥–åŠ±è¿›è¡Œå¯èƒ½çš„ä¿®è¡¥æˆ–åå¤„ç†
        self.cmt.reward = self.cmt.reward_patch(self.cmt.reward)
        
        # ç§»é™¤æœ€åä¸€æ¡ä¸Šä¸‹æ–‡ (é€šå¸¸æ˜¯ä¸ºäº†æ•´ç†æ•°æ®æ ¼å¼ï¼Œæ¯”å¦‚å»æ‰æœ€åçš„ User å›å¤ä»¥ä¾¿äºè®­ç»ƒé¢„æµ‹)
        self.cmt.remove_last_context()

        # ç”Ÿæˆæ—¥å¿— (ä½¿ç”¨é”ä¿è¯çº¿ç¨‹å®‰å…¨)
        with log_generate_lock:
            self.cmt.generate_log(task_id=task_id)  


        return self.cmt